{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929dd43b-e7cc-4343-9cc9-218f19492350",
   "metadata": {},
   "source": [
    "## StableLM local test\n",
    "\n",
    "- Test StableLM as a local mode\n",
    "- base model : https://huggingface.co/stabilityai/stablelm-base-alpha-7b\n",
    "- fine-tuned model : https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b\n",
    "- Code example : https://github.com/Stability-AI/StableLM\n",
    "\n",
    "### License Issue\n",
    "- Note that tuned model is not a commercial license. (base model is possible to use commercial purpose)\n",
    "\n",
    "### Tested version\n",
    "\n",
    "Tested on `Python 3.9.15`\n",
    "\n",
    "```\n",
    "sagemaker: 2.146.0\n",
    "transformers: 4.29.2\n",
    "torch: 1.13.1\n",
    "accelerate: 0.19.0\n",
    "sentencepiece: 0.1.99\n",
    "bitsandbytes: 0.38.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82621b09-0704-4261-a8c8-e1239f714bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers accelerate sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84857976-0ba8-4f01-bca7-10cbf142bd97",
   "metadata": {},
   "source": [
    "- Test local mode first here.\n",
    "- You can download model using git lfs, but also can use HF package\n",
    "```\n",
    "git lfs install\n",
    "git clone https://huggingface.co/stabilityai/stablelm-base-alpha-7b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae005337-4078-4dca-afb0-f2bc87fe011d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import transformers\n",
    "print(sagemaker.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d2092-f6fe-4727-91c8-cbac435c73a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "local_model_path = Path(\"./pretrained-models\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"stabilityai/stablelm-base-alpha-7b\"\n",
    "# model_name = \"stabilityai/stablelm-tuned-alpha-7b\"\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\", \"*.py\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcec3d-84dc-4639-a38b-aab5287a24ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_download_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c74c84-99eb-4611-b9a3-075d2753dfa2",
   "metadata": {},
   "source": [
    "### Instance size and model\n",
    "\n",
    "- int8 quantization consumes more than 10GB of GPU memory. `g4dn.xlarge` is possible\n",
    "- float16 needs at least `g5.2xlarge` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33ce1e-a320-4b53-bee6-b718020e6c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "model_path = model_download_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, load_in_8bit=True, device_map=\"auto\")\n",
    "# model.half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8131d3b8-0a5a-41e1-8e2f-96b6cbc8d1b5",
   "metadata": {},
   "source": [
    "### Test model inference\n",
    "\n",
    "- After loading model you can test inference.\n",
    "- Fine-tuned model needs default prompt for better performance, and for the base model you can just input simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc73e38-ee6d-4608-a97d-0186f8b096d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85fad4-2fbf-41bd-93a9-c146f4857b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [50278, 50279, 50277, 1, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746c628-17e9-4245-a30a-3f64446fda0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "# - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "# - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "# - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "# - StableLM will refuse to participate in anything that could harm a human.\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = f\"{system_prompt}<|USER|>Hi, when can I get a driver license?<|ASSISTANT|>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5bdd1-228f-4574-8c3f-6d81b746a095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt = f\"{system_prompt}\"\n",
    "# prompt += \"<|USER|>Hi, when can I get a driver license?<|ASSISTANT|>As an AI language model, I don't have access to real-time data, but typically, it would be possible to obtain a driver's license as long as you are legally eligible to drive and have the necessary documents. Some states and countries may have different regulations or requirements for obtaining a driver's license, so it's best to check with the relevant authorities for the state or country you plan to visit.\"\n",
    "# prompt += \"<|USER|>How about Japan?\\n<|ASSISTANT|>\"\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5070f4-9e76-4e01-95fb-1a85a0bdba0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Could you recommend some food at this weekend?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87852f-58f0-4eb1-9484-a29fb9761e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d283ede-8c37-4c0c-b318-5415a9725736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=256,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  # stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd55d57-1769-497d-a04e-e0a922dfa9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e3946-d518-4702-8680-6f897495f4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_model_prefix = \"llm/stablelm/model\"  # folder where model checkpoint will go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bb1e6-e7af-4661-885a-312bcd39ef3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237ab04-626e-4e83-bb7f-55048b9790ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_7b_s3 = f\"{s3_model_prefix}/base-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01611c06-497f-484b-9a76-4b3c5692e29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "stablelm_model_artifact = sagemaker_session.upload_data(path=model_download_path, key_prefix=base_7b_s3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53c726-d2b7-4ec3-9f00-905aaf4a5f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store model_download_path\n",
    "%store stablelm_model_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee37f6-4896-4df7-bd7c-8feb130319e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
