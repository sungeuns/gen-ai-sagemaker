{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250f980d",
   "metadata": {},
   "source": [
    "# RAG - FSI FAQ with FAISS Local VectorStore Test\n",
    "이 작업(TASK)에서는 앞서 생성한 SageMaker Embedding Vector 모델의 Endpoint를 활용하여 로컬 환경에서 자주 묻는 질문(FAQ) 데이터셋에 대한 FAISS(Facebook AI Similarity Search) 임베딩을 생성합니다. 이를 통해 텍스트 데이터의 유사성을 빠르게 검색할 수 있게 됩니다. 더불어, SageMaker에서 제공하는 Language Model for Learning-to-Match (LLM) Endpoint를 사용하여, 임베딩으로 검색된 정보를 기반으로 프롬프트 엔지니어링을 수행합니다. 이 과정을 통해 Retriever-Answer Generator (RAG) 로직을 테스트해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c99fe",
   "metadata": {},
   "source": [
    "## Install Install packages and Setup env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ae5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d51ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -qU pip\n",
    "    !{sys.executable} -m pip install -qU sagemaker\n",
    "    !{sys.executable} -m pip install -qU langchain\n",
    "    !{sys.executable} -m pip install -qU faiss-cpu\n",
    "    \n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409d074",
   "metadata": {},
   "source": [
    "## 1. SageMaker Endpoint Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6b633-2598-4fb9-9dcf-ea0752c9b8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 앞선 노트북에서 저장한 변수들을 로드합니다.\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09a878-2e13-4bff-9091-acfec59b41ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(endpoint_name)\n",
    "print(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c48ff9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Version fixed management during code stabilization period of langchain\n",
    "!pip install -q SQLAlchemy==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9001ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append('../utils') # src 폴더 경로 설정\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "from inference_utils import Prompter\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbe240",
   "metadata": {},
   "source": [
    "### 1.1 [한국어 Text LLM] SageMaker Kullm endpiont handler\n",
    "- Kullm Endpoint의 RequestHandler 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65f000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompter = Prompter(\"kullm\")\n",
    "params = {\n",
    "      'do_sample': False,\n",
    "      'max_new_tokens': 512, #128\n",
    "      'temperature': 1.0,  # 0.5 ~ 1.0 default = 1.0 높으면 랜덤하게 자유도. 다음 생성 문장 토큰의 자유도 \n",
    "      'top_k': 0,\n",
    "      'top_p': 0.9,\n",
    "      'return_full_text': False,\n",
    "      'repetition_penalty': 1.1,\n",
    "      'presence_penalty': None,\n",
    "      'eos_token_id': 2\n",
    "}\n",
    "\n",
    "class KullmContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        '''\n",
    "        입력 데이터 전처리 후에 리턴\n",
    "        '''\n",
    "        context, question = prompt.split(\"||SPEPERATOR||\") \n",
    "        prompt = prompter.generate_prompt(question, context)\n",
    "\n",
    "        #print (\"prompt\", prompt)\n",
    "        payload = {\n",
    "            'inputs': [prompt],\n",
    "            'parameters': model_kwargs\n",
    "        }\n",
    "                           \n",
    "        input_str = json.dumps(payload)\n",
    "        \n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        \n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))              \n",
    "        generated_text = response_json[0][0][\"generated_text\"]\n",
    "        \n",
    "        return generated_text    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ded92a",
   "metadata": {},
   "source": [
    "#### 1.1.a 기존에 deploying한 한국어생성 LLM endpoint 이름을 설정합니다. \n",
    "- endpoint_name_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279489d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aws_region = boto3.Session().region_name\n",
    "LLMTextContentHandler = KullmContentHandler()\n",
    "\n",
    "# endpoint_name_text = \"kullm-polyglot-5-8b-v2-2023-08-23-15-47-39-450-endpoint\"\n",
    "endpoint_name_text = endpoint_name\n",
    "\n",
    "seperator = \"||SPEPERATOR||\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6743adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_text = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name_text,\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=params,    \n",
    "    content_handler=LLMTextContentHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad33552",
   "metadata": {},
   "source": [
    "### 1.2 [한국어 임베딩벡터 모델] SageMaker 임베딩 벡터 모델 KoSimCSE-roberta endpiont handler\n",
    "- KoSimCSE-roberta\n",
    "\n",
    "### SagemakerEndpointEmbeddingsJumpStart클래스는 SagemakerEndpointEmbeddings를 상속받아서 작성\n",
    "\n",
    "매개변수 (Parameters):\n",
    "* texts: 임베딩을 생성할 텍스트의 리스트입니다.\n",
    "* chunk_size: 한 번의 요청에 그룹화될 입력 텍스트의 수를 정의합니다. 만약 None이면, 클래스에 지정된 청크 크기를 사용합니다.\n",
    "\n",
    "Returns:\n",
    "* 각 텍스트에 대한 임베딩의 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947c039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int=1) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "        \n",
    "        print(\"text size: \", len(texts))\n",
    "        print(\"_chunk_size: \", _chunk_size)\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            \n",
    "            #print (i, texts[i : i + _chunk_size])\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            #print (i, response, len(response[0].shape))\n",
    "            \n",
    "            results.extend(response)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6313b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KoSimCSERobertaContentHandler(EmbeddingsContentHandler):\n",
    "    \n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        \n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        \n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        \n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        ndim = np.array(response_json).ndim    \n",
    "        \n",
    "        if ndim == 4:\n",
    "            # Original shape (1, 1, n, 768)\n",
    "            emb = response_json[0][0][0]\n",
    "            emb = np.expand_dims(emb, axis=0).tolist()\n",
    "        elif ndim == 2:\n",
    "            # Original shape (n, 1)\n",
    "            emb = []\n",
    "            for ele in response_json:\n",
    "                e = ele[0][0]\n",
    "                emb.append(e)\n",
    "        else:\n",
    "            print(f\"Other # of dimension: {ndim}\")\n",
    "            emb = None\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51afa05",
   "metadata": {},
   "source": [
    "#### 1.2.a 기존에 deploying한 임베딩 벡터 endpoint 설정\n",
    "\n",
    "- endpoint_name_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f9b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLMEmbHandler = KoSimCSERobertaContentHandler()\n",
    "\n",
    "# endpoint_name_emb = \"KoSimCSE-roberta-2023-08-23-14-07-12\"\n",
    "endpoint_name_emb = embedding_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0099c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_emb = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=endpoint_name_emb,\n",
    "    region_name=aws_region,\n",
    "    content_handler=LLMEmbHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2f8d4",
   "metadata": {},
   "source": [
    "**Now, we can build an QA application. <span style=\"color:red\">LangChain makes it extremly simple with following few lines of code</span>.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e9762",
   "metadata": {},
   "source": [
    "# 2.Vector Store - FAISS\n",
    "\n",
    "### FAISS(Facebook AI Similarity Search) : \n",
    "- 밀집 벡터의 효율적인 유사성 검색과 클러스터링을 위한 라이브러리입니다. 이 라이브러리는 RAM에 들어가지 않을 수도 있는 어떤 크기의 벡터 세트에서도 검색할 수 있는 알고리즘을 포함하고 있습니다.\n",
    "- CPU(pip install faiss-cpu)와 GPU(pip install faiss-gpu) 모두에서 사용할 수 있습니다.\n",
    "- 유사성 검색: 기본적인 유사성 검색 외에도, 유사성 점수를 반환하는 similarity_search_with_score와 같은 FAISS 특화 메서드가 있습니다.\n",
    "- 벡터 기반 검색: similarity_search_by_vector 메서드를 사용하여 임베딩 벡터를 파라미터로 전달할 수 있습니다.\n",
    "- 저장 및 로딩: FAISS 인덱스를 로컬에 저장하고 로딩하는 기능이 있어, 매번 인덱스를 생성할 필요가 없습니다.\n",
    "- 벡터 스토어 병합: 두 개의 FAISS 벡터 스토어를 병합할 수 있습니다.\n",
    "- 필터링 지원: FAISS는 메타데이터를 기반으로 문서를 필터링하는 기능을 제공합니다.\n",
    "- 직렬화와 역직렬화: FAISS 인덱스를 바이트로 직렬화하고 역직렬화할 수 있습니다, 이는 데이터베이스에 인덱스를 저장할 때 유용합니다.\n",
    "\n",
    "### Vector Store :\n",
    "- Vector Store는 비정형 데이터를 임베딩 벡터로 변환하여 저장하고, 쿼리 시에 해당 임베딩 벡터와 '가장 유사한' 임베딩 벡터를 검색하는 역할을 하는 저장소입니다. 이는 텍스트 임베딩 모델과 연계하여 벡터를 생성하고 저장, 검색하는 기능을 제공합니다. 다양한 벡터 저장 옵션들이 있으며, 비동기 작업도 지원합니다.\n",
    "\n",
    "\n",
    "References\n",
    "- Vector Store : https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
    "- FAISS Vector Store : https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1a3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac405abc",
   "metadata": {},
   "source": [
    "### 2.1 Context file load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effbbf0b",
   "metadata": {},
   "source": [
    "#### 2.1.a Increase csv field size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb2c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv.field_size_limit(100000000)\n",
    "csv.field_size_limit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d7839",
   "metadata": {},
   "source": [
    "#### 2.1.b FAQ CSV 파일 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc8c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"../dataset/fsi_smart_faq_ko.csv\",\n",
    "    source_column=\"Source\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "context_documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a61d33",
   "metadata": {},
   "source": [
    "#### 2.1.c FSI FAQ Context 데이터 확인 \n",
    "- 인터넷뱅킹 FAQ > 스마트뱅킹 No.1 ~ N. 89 로 구성되었습니다. \n",
    "- https://www.shinhan.com/hpe/index.jsp#050101020000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e5a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(context_documents))\n",
    "\n",
    "context_documents[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b6a68",
   "metadata": {},
   "source": [
    "### 2.2 FAISS 벡터 Indexer 생성\n",
    "\n",
    "로드한 FAQ 컨텍스트 데이터를 Indexer를 생성 빌드합니다.\n",
    "- Recursively split by character : https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter\n",
    "\n",
    "It takes about 1 mins\n",
    "vectorstore 생성에 1분정도 소요됩니다.\n",
    "Document가 부분 절삭 되지 않도록 Chunking 전략을 잘 세웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4c4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=llm_emb,\n",
    "    #text_splitter=CharacterTextSplitter(chunk_size=700, chunk_overlap=0),\n",
    "    #text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200), \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    ")\n",
    "# 로버타 모델의 청크 크기에 종속되어있음. 청크 파라미터 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a328977",
   "metadata": {},
   "source": [
    "### 2.2.a 임베딩 벡터 스토어 파일 생성 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f21d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4aaee8",
   "metadata": {},
   "source": [
    "### 2.2.b index save and read index from local\n",
    "\n",
    "- 로컬 디렉토리에 인덱싱 데이터를 저장한 후 다시 로드해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce3bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "index.vectorstore.save_local(\"../indexer/fsi_faq_indexer_ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd6c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read\n",
    "index_ = FAISS.load_local(\"../indexer/fsi_faq_indexer_ko\", llm_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1a560",
   "metadata": {},
   "source": [
    "### 2.2.c 생성한 Indexer document 5개 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c7af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_.index_to_docstore_id\n",
    "#index_.docstore._dict[:10]\n",
    "dict(list(index_.docstore._dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7b918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(list(index_.docstore._dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ed479",
   "metadata": {},
   "source": [
    "## 3. Langchain QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37288ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8dabb",
   "metadata": {},
   "source": [
    "### 3.1 Query and Response\n",
    "\n",
    "filter_and_remove_score\n",
    "- 이 함수는 주어진 검색 결과 리스트(res)에서 특정 점수(cutoff_score와 variance를 기반으로)를 기준으로 필터링을 수행하고, 임베딩 Document 이외 검색 결과에 추가된 score를 제거합니다.\n",
    "- 본 핸즈온 데모에서는 카테고리 분류기 대신, 단순 검색 조건 필터링에 조회되지 않는 에러가 큰 (에러 거리 score가 200이상이고, 분산이 1.3이상인 값을 상한값으로 정한 다음 필터링된 문서 리스트만 반환합니다.\n",
    "- 실제 환경에서는 문장 분류 카테고리 모델의 사용을 권장합니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390eb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = ''.join([\"{context}\", seperator, \"{question}\"])\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain = load_qa_chain(llm=llm_text, chain_type=\"stuff\", prompt=PROMPT, verbose=True)\n",
    "\n",
    "\n",
    "def filter_and_remove_score(res, cutoff_score = 200, variance=1.3):\n",
    "    # Get the lowest score\n",
    "    lowest_score = min(score for doc, score in res)\n",
    "    print('lowest_score : ', lowest_score)\n",
    "    # If the lowest score is over 200, return an empty list\n",
    "    if lowest_score > cutoff_score:\n",
    "        return []\n",
    "    # Calculate the upper bound for scores\n",
    "    upper_bound = lowest_score * variance\n",
    "\n",
    "    \n",
    "    # Filter the list and remove the score\n",
    "    res = [doc for doc, score in res if score <= upper_bound]\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_similiar_docs(query, k=1, fetch_k=50, score=True, bank=\"\"):\n",
    "\n",
    "    #print (query)\n",
    "    \n",
    "    if score:         \n",
    "        similar_docs = index_.similarity_search_with_score(\n",
    "            query,\n",
    "            k=k,\n",
    "            fetch_k=fetch_k,\n",
    "            filter=dict(source=bank)\n",
    "        )\n",
    "        similar_docs=filter_and_remove_score(similar_docs)\n",
    "    else:        \n",
    "        similar_docs = index_.similarity_search(\n",
    "            query,\n",
    "            k=k,\n",
    "            fetch_k=fetch_k,\n",
    "            filter=dict(source=bank)\n",
    "        )\n",
    "        \n",
    "    return similar_docs\n",
    "\n",
    "def get_answer(query, bank=\"\", k=1):\n",
    "                \n",
    "    search_query = query\n",
    "    \n",
    "    similar_docs = get_similiar_docs(search_query, k=k, bank=bank)\n",
    "    \n",
    "    print(\"similar_docs : \", similar_docs)\n",
    "    llm_query = ''+query+' Category에 대한 Information을 찾아서 설명해주세요.'\n",
    "    \n",
    "    if not similar_docs:\n",
    "        llm_query = query\n",
    "\n",
    "    answer = chain.run(input_documents=similar_docs, question=llm_query)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "def get_search_answer (query, bank=\"\", k=1):\n",
    "    search_query = query\n",
    "    similar_docs = get_similiar_docs(search_query, k=k, bank=bank)\n",
    "    llm_query = '검색된 내용을 no. 별로 요약한 문장으로 알려줘.'\n",
    "    if not similar_docs:\n",
    "        llm_query = query\n",
    "\n",
    "    llm_answer = chain.run(input_documents=similar_docs, question=llm_query)\n",
    "    return llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fbcc9-df64-4dd0-a0c4-bd81e94b4478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.run(input_documents='', question='안녕하세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ae938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"안녕하세요. 날씨가 참 좋네요.\"\n",
    "response = get_answer(question, bank=\"신한은행\", k=5)\n",
    "\n",
    "print (f'question: {question}')\n",
    "print('==========================')\n",
    "print('\\n')\n",
    "print (f'response: \\n {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a570506",
   "metadata": {},
   "source": [
    "### 3.1.a FAQ 데이터 응답문장 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509dd3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"타기관OTP 등록 방법 알려주세요\"\n",
    "response = get_answer(question, bank=\"신한은행\", k=3)\n",
    "\n",
    "print (f'question: {question}')\n",
    "print('==========================')\n",
    "print('\\n')\n",
    "print (f'response: \\n {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdac33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"홈페이지 이용자아이디 여러 개 사용할 수 있나요?\"\n",
    "response = get_answer(question, bank=\"신한은행\", k=3)\n",
    "\n",
    "print (f'question: {question}')\n",
    "print('==========================')\n",
    "print('\\n')\n",
    "print (f'response: \\n {response}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bde7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "question = \"화면에서 미래를함께하는따뜻한금융 안전한금융거래를위해준비중입니다. 라고 나오고 맨날 멈추는데 어떻게 하냐? 아니쓸수가 없자나\"\n",
    "response = get_answer(question, bank=\"신한은행\", k=1)\n",
    "\n",
    "print (f'question: {question}')\n",
    "print('==========================')\n",
    "print('\\n')\n",
    "print (f'response: \\n {response}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1afaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "question = \"공동인증서랑 금융인증서 같이 쓸수있나요?\"\n",
    "response = get_answer(question, bank=\"신한은행\", k=3)\n",
    "\n",
    "print (f'question: {question}')\n",
    "print('==========================')\n",
    "print('\\n')\n",
    "print (f'response: \\n {response}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc43c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "question = \"홈페이지 탈퇴하는방법 알려줘\"\n",
    "response = get_answer(question, bank=\"신한은행\", k=3)\n",
    "\n",
    "print (f'question: {question}')\n",
    "print('==========================')\n",
    "print('\\n')\n",
    "print (f'response: \\n {response}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a307a-7118-48a3-899e-ac2b49699a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "question = \"질문 답변 감사합니다. 좋은 하루 되세요.\"\n",
    "response = get_answer(question, bank=\"신한은행\", k=3)\n",
    "\n",
    "print (f'question: {question}')\n",
    "print('==========================')\n",
    "print('\\n')\n",
    "print (f'response: \\n {response}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a88af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea8c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38154c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
