{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdac31bd-67ad-45c6-b85d-570185349f44",
   "metadata": {},
   "source": [
    "## RAG 실습\n",
    "\n",
    "- 앞에서 bedrock 사용법, AOSS 사용법을 모두 배웠으니 이제 기본적인 RAG가 어떻게 동작하는지 살펴볼 차례입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ba29e-a3d3-4def-b14f-f0d47202303a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426de56-bf3b-4444-b0c4-e9d2aa60eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e88819-c426-494b-96ed-9ba9c393fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(aoss_client)\n",
    "except:\n",
    "    try:\n",
    "        print(collection_name)\n",
    "        print(vector_index_name)\n",
    "        print(aoss_endpoint)\n",
    "    except:\n",
    "        collection_name = \"rag-hol-aoss-collection\"\n",
    "        vector_index_name = \"rag-hol-index-vector\"\n",
    "        aoss_endpoint = \"gtp9tx1kvlcpucvtkse6.us-west-2.aoss.amazonaws.com\"\n",
    "    \n",
    "    from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "    from requests_aws4auth import AWS4Auth\n",
    "    import botocore\n",
    "    import time\n",
    "\n",
    "    import sagemaker\n",
    "\n",
    "    sess = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "    region = boto3.Session().region_name\n",
    "\n",
    "    service = 'aoss'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                       region, service, session_token=credentials.token)\n",
    "    \n",
    "    def get_aoss_client(host):\n",
    "        client = OpenSearch(\n",
    "            hosts=[{'host': host, 'port': 443}],\n",
    "            http_auth=awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection,\n",
    "            timeout=6000\n",
    "        )\n",
    "        return client\n",
    "    \n",
    "    aoss_client = get_aoss_client(aoss_endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e959fd-6055-47fe-882b-9eb983feeb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e953883-9c7b-40c2-ac45-0b2ebdf68a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a839e1d-0ebd-477b-bf54-57e93c9e3773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0411c-f3ae-41f0-a1b4-128d8b6cea3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\"bedrock-runtime\")\n",
    "bedrock_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "def get_llm_output(prompt):\n",
    "    body = json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"temperature\" : 0.1,\n",
    "                \"top_p\": 0.5,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }) \n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, \n",
    "        modelId=bedrock_model_id,\n",
    "        accept='application/json',\n",
    "        contentType='application/json')\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    llm_output = response_body.get(\"content\")[0].get(\"text\")\n",
    "    return llm_output\n",
    "\n",
    "def get_embedding_output(query):\n",
    "    \n",
    "    body = {\n",
    "        \"inputText\": query,\n",
    "        \"dimensions\": 1024,\n",
    "        \"normalize\": True\n",
    "    }\n",
    "    \n",
    "    response = bedrock.invoke_model(\n",
    "        body=json.dumps(body), \n",
    "        modelId=embedding_model_id,\n",
    "        accept='application/json',\n",
    "        contentType='application/json')\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You're a helpful assistant to answer the question.\n",
    "Use the following pieces of <CONTEXT> to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "def get_semantic_rag(user_query):\n",
    "    vector = get_embedding_output(user_query)\n",
    "    vector_query = {\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"content_embeddings\": {\n",
    "            \"vector\": vector,\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    response = aoss_client.search(index=vector_index_name, body=vector_query, size=5)\n",
    "    vector_search_results = [result[\"_source\"][\"content\"] for result in response[\"hits\"][\"hits\"]]\n",
    "    \n",
    "    context_data = \"\\n\\n\".join(vector_search_results)\n",
    "    \n",
    "    llm_input = prompt_template.format(context=context_data, question=user_query)\n",
    "    \n",
    "    llm_output = get_llm_output(llm_input)\n",
    "    \n",
    "    return {\"llm_input\": llm_input, \"llm_output\": llm_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fac21-eac8-438e-9872-2baad3efaf08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = get_semantic_rag(\"교육에서 챗봇을 어떻게 활용해야 하나요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba304a2-9b0b-4341-8f9e-e92454a0c46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output[\"llm_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591baa6b-267a-4c4c-9d61-a2a17a1c6618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bb169-af90-4106-9b23-704894e9f235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647ec17-1f34-4307-ada6-2142ca9c5ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_normalized_result(search_results, add_meta, weight=1.0):\n",
    "    hits = search_results[\"hits\"][\"hits\"]\n",
    "    if len(hits) == 0:\n",
    "        return []\n",
    "    \n",
    "    max_score = float(search_results[\"hits\"][\"max_score\"])\n",
    "    \n",
    "    results = []\n",
    "    for hit in hits:\n",
    "        normalized_score = float(hit[\"_score\"]) / max_score\n",
    "        weight_score = normalized_score if weight == 1.0 else normalized_score * weight\n",
    "        results.append({\n",
    "            \"doc_id\": hit[\"_id\"],\n",
    "            \"score\": weight_score,\n",
    "            \"content\": hit[\"_source\"][\"content\"],\n",
    "            \"meta\": add_meta\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_hybrid_rag(user_query):\n",
    "    result_limit = 5\n",
    "    vec_weight = 0.6\n",
    "    lex_weight = 0.55\n",
    "    threshold = 0.05\n",
    "    \n",
    "    # Get vector search result\n",
    "    vector = get_embedding_output(user_query)\n",
    "    vector_query = {\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"content_embeddings\": {\n",
    "            \"vector\": vector,\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    vector_response = aoss_client.search(index=vector_index_name, body=vector_query, size=10)\n",
    "    vector_result = get_normalized_result(vector_response, \"vector\", vec_weight)\n",
    "    \n",
    "    # Get lexical search result\n",
    "    keyword_query = {\"query\": {\"match\": {\"content\": query_text}}}\n",
    "    keyword_response = aoss_client.search(index=vector_index_name, body=keyword_query, size=10)\n",
    "    keyword_result = get_normalized_result(keyword_response, \"lexical\", lex_weight)\n",
    "    \n",
    "    vector_ids = [vec[\"doc_id\"] for vec in vector_result]\n",
    "    for keyword in keyword_result:\n",
    "        if keyword[\"doc_id\"] not in vector_ids:\n",
    "            vector_result.append(keyword)\n",
    "    \n",
    "    items = vector_result\n",
    "    sorted_items = list(filter(lambda val: val[\"score\"] > threshold, items))\n",
    "    \n",
    "    if len(sorted_items) > result_limit:\n",
    "        sorted_items = sorted_items[:result_limit]\n",
    "    \n",
    "    context_data = \"\\n\\n\".join([item[\"content\"] for item in sorted_items])\n",
    "    llm_input = prompt_template.format(context=context_data, question=user_query)\n",
    "    llm_output = get_llm_output(llm_input)\n",
    "    return {\"llm_input\": llm_input, \"llm_output\": llm_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00123a5-b65c-4c2c-ab55-12e3f952d4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12f527-5a4d-4e9b-b4e7-5de11fcfe9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_text = \"교육에서 챗봇을 어떻게 활용해야 하나요\"\n",
    "query_text = \"How to use chatbot for education?\"\n",
    "output = get_hybrid_rag(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929d1d6-7056-40d0-8945-f04596b0cf21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output[\"llm_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54746e4-999f-423b-9b0c-4897e456ab6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ebc3d-8fe7-4b3b-8cf2-685b987ea233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
