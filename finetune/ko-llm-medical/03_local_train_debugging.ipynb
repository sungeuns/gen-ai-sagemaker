{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe53407-734a-40e6-adfa-b63421367842",
   "metadata": {},
   "source": [
    "## Local training 테스트\n",
    "\n",
    "- SageMaker managed training을 하기에 앞서 local 환경에서 training을 진행해 보도록 합니다.\n",
    "\n",
    "### QLoRA 활용\n",
    "\n",
    "- GPU memory가 적더라도 fine-tuning이 가능하며, 이는 LoRA와 같은 PEFT 를 활용해서 가능합니다.\n",
    "- 여기서는 4bit quantization을 활용하는 QLoRA 를 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf933ea-acea-44f4-b697-99db13443794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe22f5b-affe-4bb8-8d5d-fc92d5d98404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807337d-32c2-4a94-a8dc-991e9e84a1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270a083-2461-4012-8928-1aeed810cf07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quant_4bit = True\n",
    "quant_8bit = False\n",
    "\n",
    "if quant_4bit:\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    nf4_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eacccc-9d92-47a2-b2bc-a59be2d8941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f1210-9000-4e8a-8077-347dda4af3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_map = \"auto\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_download_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_download_path,\n",
    "    load_in_8bit=True if quant_8bit else False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    quantization_config=nf4_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea88113-623b-4414-88be-fb047dea571f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4cff29-55b3-4244-a403-a84a72c596fd",
   "metadata": {},
   "source": [
    "### LoRA 설정\n",
    "\n",
    "- LoRA 설정은 [여기](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)를 참고해 주세요\n",
    "- Target module의 경우 base model 이 무엇이냐에 따라 다를 수 있습니다. 여기서는 base model 이 solar 기반이고 이것은 llama 기반이기 때문에 llama 의 설정을 참고하였습니다. 관련된 내용은 [여기](https://www.reddit.com/r/LocalLLaMA/comments/1578ahb/target_modules_for_llama2_for_better_finetuning/) 를 참고해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad33a70-31d8-4651-948a-8cdd27bd6c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_r  = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\"\n",
    "  ]\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a29ac7-96fc-48e9-8708-8a05b6e6e48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f713f-76cc-4a93-bcfb-d215467c378f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "train_data = load_from_disk(os.path.join(\"dataset\", \"train\"))\n",
    "val_data = load_from_disk((os.path.join(\"dataset\", \"val\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9093e2-789c-45f9-b085-3bf4dbb0de28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Val set size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ec4f6-9df5-49c1-9d76-312d9af4943a",
   "metadata": {},
   "source": [
    "### 로컬 학습 진행\n",
    "\n",
    "- 학습과 관련된 파라미터를 세팅하고 학습을 진행합니다.\n",
    "- bfloat16과 같은 precision은 지원하는 GPU에서는 활용하도록 하여 좀 더 효율적으로 할 수 있습니다.\n",
    "- HuggingFace transformers의 [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) 를 사용하면 변수값만 수정하여 쉽게 학습을 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf8df5-e325-4538-b29c-5951cc23aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "learning_rate = 3e-5\n",
    "gradient_accumulation_steps = 2\n",
    "val_set_size = len(val_data)\n",
    "output_dir = 'output'\n",
    "world_size = 1\n",
    "ddp = world_size != 1\n",
    "group_by_length = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1e240-26df-42f9-a891-0c5daef58c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bf16 = True if torch.cuda.get_device_capability()[0] == 8 else False\n",
    "print(f\"Use bfloat16: {bf16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947799e3-2008-41c3-844d-e222ff0a4631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=bf16,  # g4dn (Nvidia T4) cannot use bf16\n",
    "    logging_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=20 if val_set_size > 0 else None,\n",
    "    save_steps=40,\n",
    "    output_dir=output_dir,\n",
    "    load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "    ddp_find_unused_parameters=False if ddp else None,\n",
    "    report_to=\"none\",\n",
    "    group_by_length=group_by_length,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6afb22-86f8-4a46-b4fa-ee10c78a29db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=train_args,\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd30b8-2d26-4070-944b-059b8851f4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "#     model, type(model)\n",
    "# )\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f6850-bde1-4433-b803-2e41ca1d516b",
   "metadata": {},
   "source": [
    "### 학습이 완료된 경우\n",
    "\n",
    "- 학습이 완료되면 모델 머지가 필요합니다. 왜냐면 PEFT의 LoRA를 사용했기 때문에, fine-tuning 한 adapter를 base model 에 머지하는 과정이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa53cf-f5f7-4fd8-a28c-d80c099232cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "#trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737dc42-3fff-4d04-a7fa-0cd7e065e551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e285d0d-b2a8-4c55-b310-361425151899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a2881-48a5-4b39-9b17-ff02e98d6717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61b085-1031-4689-b603-eaa11cb92632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581dd94-d319-45ac-a469-b8a2a7c60f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e7fca-5d8f-46b9-a8b0-8a792b69e131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_path = \"merged_model\"\n",
    "os.makedirs(merged_path, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_path, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33da0af-6d30-40c6-9ac3-2f1a37ff25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store merged_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03986bf-93dc-4eaa-b667-22f20405eaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
