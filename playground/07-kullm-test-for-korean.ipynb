{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89c79ec-d6fd-4aa6-8933-b95e993c84b9",
   "metadata": {},
   "source": [
    "## Kullm : 한국어를 위한 LLM\n",
    "\n",
    "- github : https://github.com/nlpai-lab/KULLM\n",
    "- hf model (12b model) : https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\n",
    "- 6b model : https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b288187-05b6-4b1e-aaa6-470b74531690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c66a8a-32c3-435f-910b-3bdc99c46f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09c141-828d-4544-b508-d0a5a6ee0424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import transformers\n",
    "print(sagemaker.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0c158-a54d-4391-83c2-c3ba8c23a828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "local_model_path = Path(\"./pretrained-models\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "# model_name = \"nlpai-lab/kullm-polyglot-5.8b-v2\"\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\", \"*.py\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a00df6-cd96-4d9c-b15e-8fe5407c14c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Local model download path: {model_download_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087c480-b432-40a7-94af-7e4e029ab989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f64653-40b2-40bf-ae1b-05efab08f17e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_download_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_download_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2f96c-48a2-4960-8ead-5cf07f47219a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbed3a-23b0-4660-9668-35f5673c8929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer(instruction=\"\", input_text=\"\"):\n",
    "    prompt_format = \"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n{instruction}\\n\\n### 입력:\\n{input}\\n\\n### 응답:\\n\"\n",
    "    prompt = prompt_format.format(instruction=instruction, input=input_text)\n",
    "    \n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_length=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.7,\n",
    "        eos_token_id=2\n",
    "    )\n",
    "    s = output[0][\"generated_text\"]\n",
    "    result = s.split(\"### 응답:\")[1].strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06c6d5-ef33-4f48-a42f-1a23d44ebe1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71035d9-bc82-4444-b1db-1e2946b7e7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = infer(input_text=\"새로 태어나는 아이를 위한 이름을 5개 추천해줘\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaeaffb-7c6b-499a-8986-668302cfb5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = infer(input_text=\"삼성서울병원에 어떻게 가야되?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a742540-f6b4-4107-9f3a-10ba5da258bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = infer(input_text=\"프롬프트 엔지니어링 잘 하는 법좀 알려주세요\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b566b-c926-4ef5-a376-bd5ff7494b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To check eos token => <|endoftext|> : 2\n",
    "# tokenizer(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529f7d3-7b41-4cc0-b5ce-1f71b8ddcbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Release the model (clear cuda memory)\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c56a0-c8f9-4599-becb-d3868bdfd8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a23ed22a-0b9a-486b-8d64-6de208916ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### vLLM 을 활용한 테스트\n",
    "\n",
    "vLLM을 사용하여 inference speed를 얼마나 높일 수 있는 지 테스트 진행\n",
    "- vllm 0.1.1 기준으로 kullm 모델을 지원하지 않습니다. (kullm의 attention head size 가 256인데 지원되지 않음.)\n",
    "- 이것은 코드를 수정해서 해결할 수 있습니다. 여기 이슈 (https://github.com/vllm-project/vllm/issues/302) 처럼 코드 수정 후 다시 빌드해서 하면 정상적으로 동작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03e0c2-d31d-4df8-8b45-1717a074c20f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b198e39-ed21-4a35-9565-824bd370ad9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vllm.vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=model_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c3b93-306a-4a7c-af69-771bacab5242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    # max_tokens=128,\n",
    "    max_tokens=512,\n",
    "    stop=[\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cfd2f-2a9b-4a96-ac1e-5f6d29bf7d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vllm_infer(instruction=\"\", input_text=\"\"):\n",
    "    prompt_format = \"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n{instruction}\\n\\n### 입력:\\n{input}\\n\\n### 응답:\\n\"\n",
    "    prompt = prompt_format.format(instruction=instruction, input=input_text)\n",
    "    \n",
    "    output = llm.generate([prompt], sampling_params)\n",
    "    generated_text = output[0].outputs[0].text\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862082b-559c-4223-8c20-8139e88b0726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = vllm_infer(input_text=\"프롬프트 엔지니어링 잘 하는 법좀 알려주세요\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442876c8-6860-4910-9afa-796f89e6a3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = vllm_infer(input_text=\"광주광역시 근처에 여행할 만한 곳좀 추천해 주세요.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa2ae4-daf4-4cb6-8b01-2bc502f90386",
   "metadata": {},
   "source": [
    "### 테스트 결과\n",
    "- 비슷한 parameter와 output token size에 그냥 HF transformers로 로딩했을 때와 비교해서 2배 가까이 inference 속도의 개선이 있는 것을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ff3bc-16d4-474a-b029-5f8fdd38344a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
