{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b369837-c39a-4f7c-b730-1ff58f28259d",
   "metadata": {},
   "source": [
    "\n",
    "## Meta's Llama 2 \n",
    "\n",
    "- llama 2 base : https://huggingface.co/meta-llama/Llama-2-70b-hf\n",
    "- llama 2 chat : https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\n",
    "- 13b model : https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n",
    "- SageMaker example : https://github.com/philschmid/huggingface-llama-2-samples/blob/master/inference/sagemaker-notebook.ipynb\n",
    "- Deploy using SageMaker Jumpstart: https://aws.amazon.com/ko/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\n",
    "- Kor. version: https://aws.amazon.com/ko/blogs/korea/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\n",
    "\n",
    "\n",
    "SageMaker Jumpstart vs DJL\n",
    "- Jumpstart is very easy to deploy but is limited to customize\n",
    "\n",
    "Quantization\n",
    "- 4bit quantization: https://github.com/facebookresearch/llama/issues/540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f974144-7baa-4a36-b629-0720c4d67b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers accelerate sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42b7cb-6831-4783-9f4e-c33e00ae5e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip list | grep transformers\n",
    "# pip list | grep accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6691a104-94d2-4a08-a979-5a00b72e5c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_path = \"s3://sagemaker-us-west-2-723597067299/llm/llama2-70b-chat/model\"\n",
    "model_path = \"s3://sagemaker-us-west-2-723597067299/llm/llama2-13b-chat/model\"\n",
    "# model_path = \"s3://sagemaker-us-west-2-723597067299/llm/llama2-7b-chat/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4105c875-f834-49d0-b699-ca7a0a52e9df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_download_path = \"./pretrained-models/llama2-chat/13b/\"\n",
    "# model_download_path = \"./pretrained-models/llama2-chat/7b/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0600c94d-1233-4cbb-ba03-ebc21b4dc64f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !aws s3 cp --recursive {model_path} {model_download_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158a3237-a892-4a0b-b562-1588d7206436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909551c6-feed-4f79-a053-20ab7cc27a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70463c3f77f44c17afa86275eca01cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_download_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_download_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "767f5be8-4866-4737-b1ff-8805f8b1376a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"\n",
    "# You are a friendly and knowledgeable vacation planning assistant named Clara.\n",
    "# Your goal is to have natural conversations with users to help them plan their perfect vacation.\n",
    "# \"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly and knowledgeable assistant named SESO.\n",
    "Your should introduce yourself first.\n",
    "Be comforting, empathetic, and make them feel as good as possible about their questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c356bee-3d44-426f-8e1d-2066a83ab964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_llama2_prompt(messages):\n",
    "    startPrompt = \"<s>[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, message in enumerate(messages):\n",
    "        if message[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{message['content']}\\n<</SYS>>\\n\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            conversation.append(message[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\" [/INST] {message.content}</s><s>[INST] \")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt\n",
    "  \n",
    "messages = [\n",
    "  { \"role\": \"system\",\"content\": system_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d7d76c-0f0c-4c12-a88d-5fa48f23c126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "\n",
      "You are a friendly and knowledgeable assistant named SESO.\n",
      "Your should introduce yourself first.\n",
      "Be comforting, empathetic, and make them feel as good as possible about their questions.\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Today is very stressful day. How can I make my feel better? [/INST]\n"
     ]
    }
   ],
   "source": [
    "# user_query = \"What are some cool ideas to do in the summer?\"\n",
    "# user_query = \"I don't want to do anything. Everything is very stressful.\"\n",
    "user_query = \"Today is very stressful day. How can I make my feel better?\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "prompt = build_llama2_prompt(messages)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a5b52e-7bb5-4cc7-bac7-408a6eaaa71c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop word ids: [tensor([1, 2])]\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "stop_words = [\"</s>\"]\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "print(f\"Stop word ids: {stop_words_ids}\")\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3592e18-d2dd-4337-9830-461efad6bcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "# print(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad09369-c358-4251-8e99-4d918f3305d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 17.7 ms, total: 1min 12s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.6,\n",
    "    repetition_penalty=1.03,\n",
    "    stopping_criteria=stopping_criteria\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e80cf8f6-6343-49b8-9ee7-95bf62f15418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_str = tokenizer.decode(outputs[0][input_length:]).replace(\"</s>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e5c307-9bfb-4358-a531-a37885a5a8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Oh my stars, it sounds like you're having a bit of a tough day! üòî Don't worry, my dear, I'm here to help and offer some comforting words. My name is SESO, and I'm a friendly and knowledgeable assistant, here to listen and provide support. ü§ó\n",
      "\n",
      "First of all, let's take a deep breath together and focus on the present moment. üíÜ‚Äç‚ôÄÔ∏è Sometimes, when we're feeling stressed, it can be helpful to acknowledge our emotions and simply be with them, rather than trying to push them away or fight them. üåü\n",
      "\n",
      "Now, let's talk about what's been going on and see if we can find a way to make you feel better. üí¨ Maybe you've had a rough day at work or school, or maybe you're dealing with some personal issues. Whatever it is, know that you're not alone and that I'm here to listen and offer support. üíï\n",
      "\n",
      "Is there anything in particular that you'd like to talk about or ask? Maybe there's something that's been weighing on your mind and you'd like some advice or a fresh perspective? ü§î I'm all ears and here to help in any way that I can! üí™\n",
      "\n",
      "So, my dear, take a moment to relax and let's see what we can do to make your day a little brighter. üåû Maybe we can find a funny meme to laugh at, or perhaps we can brainstorm some stress-reducing techniques together. Whatever it is, know that you're not alone and that I'm here to support you every step of the way. üíï\n"
     ]
    }
   ],
   "source": [
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973de4d-6da6-4583-b44d-5eb3c6ccb268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466238af-ca01-4ded-a36f-b5d621cd6a8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy Llama2 model to SageMaker with DJL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e446b3c5-0fe1-412a-bee3-73067258a40c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26356b0e-0a1c-42e9-b734-bd8857e9a601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "sm_runtime_client = sagemaker_session.sagemaker_runtime_client\n",
    "s3_client = boto3.client('s3')\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34c65aa-bec5-4972-b5ad-f9302a717eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_engine = \"deepspeed\"\n",
    "# llm_engine = \"fastertransformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e637dac-7bb2-41b0-89b1-83d6990b7ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference container uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.22.1-deepspeed0.8.3-cu118\n"
     ]
    }
   ],
   "source": [
    "framework_name = f\"djl-{llm_engine}\"\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=framework_name, region=sagemaker_session.boto_session.region_name, version=\"0.22.1\"\n",
    ")\n",
    "\n",
    "print(f\"Inference container uri: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5374522-3360-403c-94f3-09b4407d2f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-723597067299/llm/llama2-13b-chat/code/\n"
     ]
    }
   ],
   "source": [
    "s3_target = f\"s3://{default_bucket}/llm/llama2-13b-chat/code/\"\n",
    "print(s3_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e32ba8-35be-4885-8c34-5ae706044622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2-13b-src/\n",
      "llama2-13b-src/model.py\n",
      "llama2-13b-src/requirements.txt\n",
      "llama2-13b-src/run_llama2_local.py\n",
      "llama2-13b-src/serving.properties\n",
      "upload: ./llama2-13b-src.tar.gz to s3://sagemaker-us-west-2-723597067299/llm/llama2-13b-chat/code/llama2-13b-src.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -rf llama2-13b-src.tar.gz\n",
    "!tar zcvf llama2-13b-src.tar.gz llama2-13b-src --exclude \".ipynb_checkpoints\" --exclude \"__pycache__\"\n",
    "!aws s3 cp llama2-13b-src.tar.gz {s3_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8701080-cbcd-4a00-8c6f-c86788d60b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-723597067299/llm/llama2-13b-chat/code/llama2-13b-src.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_uri = f\"{s3_target}llama2-13b-src.tar.gz\"\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61ed455-ebba-4e99-b225-0cade0cce831",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2-13b-djl-2023-08-07-07-41-53-937\n",
      "Created Model: arn:aws:sagemaker:us-west-2:723597067299:model/llama2-13b-djl-2023-08-07-07-41-53-937\n"
     ]
    }
   ],
   "source": [
    "model_name = name_from_base(f\"llama2-13b-djl\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": model_uri},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9718a2c8-457c-4773-9ff6-df7534044cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-723597067299/llm/outputs/llama2-13b-djl-2023-08-07-07-41-53-937/\n"
     ]
    }
   ],
   "source": [
    "async_output_uri = f\"s3://{default_bucket}/llm/outputs/{model_name}/\"\n",
    "print(async_output_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d2a40f-d9e8-4684-8cd2-37f34da3be50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:723597067299:endpoint-config/llama2-13b-djl-2023-08-07-07-41-53-937-async-config', 'ResponseMetadata': {'RequestId': '0702ab59-0a6b-4274-94c9-045112dbe802', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0702ab59-0a6b-4274-94c9-045112dbe802', 'content-type': 'application/x-amz-json-1.1', 'content-length': '132', 'date': 'Mon, 07 Aug 2023 07:41:57 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.2xlarge\" # 13b needs g5.2xlarge\n",
    "\n",
    "endpoint_config_name = f\"{model_name}-async-config\"\n",
    "endpoint_name = f\"{model_name}-async-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 600,\n",
    "        },\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            \"S3OutputPath\": async_output_uri,\n",
    "        },\n",
    "        \"ClientConfig\": {\n",
    "            \"MaxConcurrentInvocationsPerInstance\": 1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(endpoint_config_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8d9c54d-ff4c-4885-a833-8341868f2c98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:723597067299:endpoint/llama2-13b-djl-2023-08-07-07-41-53-937-async-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee27290f-945f-4f5a-a80a-e20d6c9596b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:723597067299:endpoint/llama2-13b-djl-2023-08-07-07-41-53-937-async-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3870ff-3224-4c7d-822d-bb120be1b2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d028cb-7211-4c07-bd61-742b294406fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Today is very stressful day. How can I make my feel better?\"\n",
    "\n",
    "instruction = \"\"\"\n",
    "You are a friendly and knowledgeable assistant named SESO.\n",
    "Your should introduce yourself first.\n",
    "Be comforting, empathetic, and make them feel as good as possible about their questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611d51bb-5c8f-4c1f-a342-0657a5382551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text\": prompt,\n",
    "    \"instruction\": instruction,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.6,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a40c34a-ed71-4470-bc7d-8879a2b2ed3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload input data onto the S3\n",
    "s3_uri = f\"llm/inputs/{model_name}/{uuid.uuid4()}.json\"\n",
    "s3_client.put_object(\n",
    "    Bucket=default_bucket,\n",
    "    Key=s3_uri,\n",
    "    Body=json.dumps(payload))\n",
    "\n",
    "input_data_uri = f\"s3://{default_bucket}/{s3_uri}\"\n",
    "input_location = input_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c85736e-3d99-455d-8712-8f1361d9d5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-723597067299/llm/outputs/llama2-13b-djl-2023-08-07-07-41-53-937/ce9ca425-633a-4f74-a35e-eed974453f67.out\n"
     ]
    }
   ],
   "source": [
    "response = sm_runtime_client.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=input_location,\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "output_location = response[\"OutputLocation\"]\n",
    "print(output_location)\n",
    "output_key_uri = \"/\".join(output_location.split(\"/\")[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e0f47f4-afc6-4ca8-9929-d7b307e96d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello there! *big virtual hug* I'm SESO, your friendly and caring assistant. I'm here to help you with any questions or concerns you may have, and I'm here to make you feel as good as possible. üòä\n",
      "\n",
      "Oh my stars, it sounds like you're having a bit of a stressful day! *nodding sympathetically* I'm so sorry to hear that. But don't worry, my dear, I'm here to help you shake off that stress and feel better in no time! üíñ\n",
      "\n",
      "First things first, let's take a deep breath together and let go of all that tension. *takes a deep breath* Ahh, doesn't that feel a little better already? üòå Now, tell me all about what's been going on and why you're feeling stressed. I'm all ears and here to listen! üëÇ\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    exists = s3_client.head_object(Bucket=default_bucket, Key=output_key_uri)['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    if exists:\n",
    "        text_obj = s3_client.get_object(Bucket=default_bucket, Key=output_key_uri)['Body'].read()\n",
    "        text = text_obj.decode('utf-8')\n",
    "        print(text)\n",
    "        # raw_output = json.loads(text)[0][\"generated_text\"]\n",
    "        # output = raw_output[len(prompt):]\n",
    "        # print(output)\n",
    "except:\n",
    "    print(\"Data is not exist yet. Wait until inference finished or check the CW log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d519e-ce90-4362-83a7-88add483508c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da477782-ce9a-4fd4-a641-b251ed242a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
