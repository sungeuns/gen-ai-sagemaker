{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e28a4aa-f042-4f1c-b842-7e4594f944b6",
   "metadata": {},
   "source": [
    "## DJL을 활용한 배포\n",
    "\n",
    "- DJL은 SageMaker에서 LLM을 배포하는 용도로 만들어진 container image입니다. 기본적으로 DeepSpeed, FasterTransformer를 활용할 수 있습니다.\n",
    "- LLM을 배포할 때는 일반적인 `HuggingFaceModel` 객체를 사용한 HuggingFace 기본 DLC 보다 DJL을 사용하는 것이 좋습니다. 모델 로딩 시 30GB 가 넘어가면 EBS volume 이 충분함에도 root volume 크기 문제로 제대로 로딩이 안될 수 있기 때문입니다. (Training은 상관없습니다.)\n",
    "- 공식 문서의 튜토리얼에 잘 설명이 되어 있습니다 : https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-tutorials.html\n",
    "- DJL을 활용한 배포 예시 : https://github.com/dhawalkp/dolly-12b/blob/main/dolly-12b-deepspeed-sagemaker.ipynb\n",
    "- DJL의 deepspeed 기본 inference 코드 : https://github.com/deepjavalibrary/djl-serving/blob/master/engines/python/setup/djl_python/deepspeed.py\n",
    "\n",
    "\n",
    "### Container that used for deployment\n",
    "- deepspeed: `763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117`\n",
    "- fastertransformer: `763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.21.0-fastertransformer5.3.0-cu117`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bd9c5-2e8d-4aa4-b0f3-02c1403da6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5249ac-0312-471c-84c9-49699cf576e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499e5c5-b030-4b53-993d-a01b0d6ad1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784238c-6ce1-4866-9d4c-3e485d54d463",
   "metadata": {},
   "source": [
    "### DJL container 이미지 선택\n",
    "\n",
    "- 미리 준비된 DJL 이미지를 선택합니다. [여기](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html)에서 사용 가능한 이미지 리스트와 어떤 라이브러리 버전을 활용하고 있는지 확인이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8a010-7acc-41ae-97eb-6cd9fdc961f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_engine = \"deepspeed\"\n",
    "# llm_engine = \"fastertransformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ccd25-a66e-4073-b00f-049cb45044d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "sm_runtime_client = sagemaker_session.sagemaker_runtime_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b742843-0267-472f-8619-30ad19e4ff8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "framework_name = f\"djl-{llm_engine}\"\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=framework_name, region=sagemaker_session.boto_session.region_name, version=\"0.21.0\"\n",
    ")\n",
    "\n",
    "print(f\"Inference container uri: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178f039-728c-4bf1-8785-24852dc876e8",
   "metadata": {},
   "source": [
    "### DJL inference 코드 작성\n",
    "\n",
    "DJL 사용은 간단합니다. 실제 inference를 위한 `model.py` 파일 (기본 inference script 사용 시 이것도 필요없습니다.) 과 `serving.properties` 파일만 작성해 주면 됩니다. 구조는 복잡하지 않기 때문에 `dolly-src` 에 있는 예시를 참고해 주세요.\n",
    "- 옵션에 관련해서는 [여기](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html) 를 참고해 주세요.\n",
    "- 아래와 같은 구조를 갖게 됩니다. 해당 디렉토리를 tar.gz 압축 후 s3에 업로드하도록 합니다.\n",
    "\n",
    "```\n",
    "- dolly-src\n",
    "  - instruct_pipeline.py\n",
    "  - model.py\n",
    "  - serving.properties\n",
    "```\n",
    "\n",
    "\n",
    "### 변경 필요한 부분\n",
    "\n",
    "- 업로드 된 모델의 s3 주소 (`model_artifact`) 에 맞추어서 `serving.properties` 의 `option.s3url`을 수정해 주어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121209c4-c90b-4f30-9372-9d0efe85ddbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_target = f\"s3://{sagemaker_session.default_bucket()}/llm/databricks/dolly-v2-7b/code/\"\n",
    "print(s3_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c305a3-792c-4199-b0b4-43e5efb626ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm dolly-src.tar.gz\n",
    "!tar zcvf dolly-src.tar.gz dolly-src --exclude \".ipynb_checkpoints\" --exclude \"__pycache__\"\n",
    "!aws s3 cp dolly-src.tar.gz {s3_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7f8e7-21dc-4ede-ab11-f6d4fb79f055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_uri = f\"{s3_target}dolly-src.tar.gz\"\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda80f31-ef1c-4d22-b7df-5fb426d1c6fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 모델 배포\n",
    "\n",
    "- container image 는 DJL을 사용할 것이고, 코드를 압축해서 s3에 올렸기 때문에 이제 모델을 배포할 차례입니다.\n",
    "- `model 생성 -> endpoint config 생성 -> endpoint 생성` 순서로 진행하면 되며, 해당 과정은 SageMaker console에서 모두 확인이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10bad4-9e4d-4079-b906-56155a89cd35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = name_from_base(f\"dolly-7b-djl\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": model_uri},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15b906-5890-42aa-8406-24eb3ac2e6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instance_type = \"ml.g5.4xlarge\"\n",
    "instance_type = \"ml.g4dn.2xlarge\"  # dolly-7b 배포시 최소 사양\n",
    "# instance_type = \"ml.g5.12xlarge\"  # multi-gpu 가 필요한 모델의 경우\n",
    "\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 600,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(endpoint_config_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bfb07-1c04-4204-969c-ea93a061189f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513242f-76f9-4b14-9e22-fd808bcd6c12",
   "metadata": {},
   "source": [
    "### 배포 시작\n",
    "\n",
    "- Endpoint 생성 요청을 하면 EC2를 할당받고, s3에서 모델을 다운받아서 준비상태가 됩니다.\n",
    "- 배포는 10분 이상 걸릴 수 있습니다.\n",
    "- `Status`가 `InService` 가 되면 정상적으로 배포가 된 상태입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c3599-3ce5-4a47-8c92-f582e2a5df06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d70b5-31d0-4792-a2b9-1c321c8ac1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aaa12e-c4ad-4224-afe6-2bcc66b2d9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Explain to me how to use aws serverless services\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7264d3c-593d-4160-b509-c2f02960c888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompts = [prompt]\n",
    "response_model = sm_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"text\": prompts,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6a273-3f45-4177-8a5e-5623fca4a3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = str(response_model[\"Body\"].read(), \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988dbac-f672-49d6-9896-8904d5e8d768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = json.loads(output)[0][0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e0d7d-7104-4a8f-8312-22edc8df1c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae830958-6f58-4c14-bc65-ba52d3ecd013",
   "metadata": {},
   "source": [
    "### 결과\n",
    "\n",
    "- 텍스트 생성이 잘 동작하는 걸 확인할 수 있습니다.\n",
    "- SageMaker Endpoint는 여러 기능 (Load balancing, Autoscaling, ...) 이 있기 때문에 이를 활용하여 다양한 LLM 모델을 쉽게 배포할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c07fb-5701-4c01-a1f6-c83d5eb26e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5351a7c2-e3e3-42aa-8c02-276f853197f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4c67a-6d3b-4873-bcfa-16b459309ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a06691-fc7c-4519-bdff-541292f7a4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
