{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4208b543-722e-47d6-ba03-3fc47fd4936f",
   "metadata": {},
   "source": [
    "## SFT (Supervised Fine Tuning) 을 통해 특정한 Task 성능을 향상시키기\n",
    "\n",
    "- LLM을 사용하는 것은 쉽지만, 원하는 성능이 나오는 것은 쉽지 않습니다. 따라서 이 경우 Fine-tuning을 진행하게 됩니다.\n",
    "- 이 때 LLM 모델 전체를 fine-tuning하는 것은 너무 많은 리소스가 필요하기 때문에, 최소한의 리소스로 최대의 성능을 내는 fine-tuning 기법들이 많이 등장하였고, 대표적으로 [LoRA](https://arxiv.org/abs/2106.09685) 와 같은 알고리즘이 있습니다.\n",
    "- 이를 쉽게 활용할 수 있도록 한 HuggingFace의 [PEFT](https://github.com/huggingface/peft) 라이브러리를 활용하면 쉽고 빠르게 Fine-tuning이 가능합니다.\n",
    "- 이 예시에서는 HF의 BLOOM 모델을 PEFT로 fine-tuning 하는 [블로그](https://www.philschmid.de/bloom-sagemaker-peft)를 참고하였습니다.\n",
    "- 코드 참고 : https://github.com/huggingface/notebooks/tree/main/sagemaker/24_train_bloom_peft_lora\n",
    "\n",
    "\n",
    "### Tested version\n",
    "\n",
    "Tested on `Python 3.9.15`\n",
    "\n",
    "```\n",
    "sagemaker: 2.146.0\n",
    "transformers: 4.29.2\n",
    "torch: 1.13.1\n",
    "accelerate: 0.19.0\n",
    "datasets: 2.12.0\n",
    "py7zr: 0.20.5\n",
    "peft: 0.3.0\n",
    "bitsandbytes: 0.38.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740b5b9-54c8-488e-aee3-361d2a791e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516eb530-c67b-4f16-b22b-5fd51467c175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import sagemaker\n",
    "print(transformers.__version__)\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31b78e-27db-417a-8e4a-45b51429cc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "sm_runtime_client = sagemaker_session.sagemaker_runtime_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa232fe-ab16-4169-bd40-378cab019447",
   "metadata": {},
   "source": [
    "### 데이터셋 다운로드\n",
    "\n",
    "- 주어진 사람들의 대화에 대해서 요약을 하는 [samsum dataset](https://huggingface.co/datasets/samsum) 을 활용할 것입니다.\n",
    "- 데이터를 다운로드 받은 후 tokenize 하여 s3에 올려놓도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc74844-47ed-41f3-9757-516b7509900c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56deb09a-7b14-40f6-9413-9a64e65399aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"samsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fd42c-4bba-43a9-b8e5-f46653bbb4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Training dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474bebb9-b65d-4382-80fc-314fc274e0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d634a0-8746-4003-9c53-39c31a6f409b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15225b0-4534-467b-9d28-5009c080d712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_location = model_download_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_location, padding_side=\"left\")\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26bbfb-f3c6-4d16-8a0a-ef32223cc747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d439c-b16d-487f-ab79-91ec8790c295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e0165-17b6-4c1f-86a6-8b085966fc35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e166d6-f6eb-4159-b541-2e4c5d3bd01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = prompt_template.format(dialogue=sample[\"dialogue\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f66ca8-03e8-4127-bd90-e51bb225e1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc899030-fa7a-41ee-9d2b-5c0202241949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset[randint(0, len(dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9db49f-1879-48c2-a29f-f63f0661a14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remainder = {\"input_ids\": [], \"attention_mask\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcf75c-d79f-4207-9ef6-74c2018e5fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e445fe-e069-46a2-8eb5-f6d34af178e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360de56-29ed-43d3-8b83-4b40f051e093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bcb9c-9123-4447-9a91-67d82283a451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_input_path = f\"s3://{sagemaker_session.default_bucket()}/llm/databricks/dolly-v2-7b/dataset/samsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6925fc-ce10-47e3-a67c-aa23d6225350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_dataset.save_to_disk(training_input_path)\n",
    "print(f\"Data uploaded : {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46205a-8941-446d-8e63-2fc0173244d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_dataset.save_to_disk(\"./samsum-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846a830-052d-4623-8af8-3d4f0ebf554f",
   "metadata": {},
   "source": [
    "### Fine-tuning 을 위한 코드 작성 및 SageMaker managed training\n",
    "\n",
    "- training data를 업로드 하였으니, training을 위한 코드를 작성해야 합니다.\n",
    "- 예시 코드를 학습할 때 필요한 패키지들은 아래와 같습니다. 예시 코드는 `sft-src` 디렉토리를 참고해 주세요.\n",
    "- 자세한 버전은 위의 tested version 부분을 확인해 주세요.\n",
    "```\n",
    "- transformers (4.27 이상 - 그 전 버전은 int8 학습 지원안됨)\n",
    "- peft\n",
    "- datasets\n",
    "- bitsandbytes\n",
    "- accelerate\n",
    "```\n",
    "- SageMaker의 기본 HuggingFace DLC에서 이 글 작성 시점에 transformers 버전을 4.26까지만 지원하기 때문에, requirements.txt 에 그 이상 버전이 필요한 경우 버전을 명시해서 설치할 수 있습니다.\n",
    "- `sft-src` 디렉토리에 있는 코드는 블로그 원본 코드에서 일부 수정이 되었습니다. 예를 들어 기존에는 pretrained model을 HuggingFace Model hub에서 가져오는데 이것은 속도도 더 느리고 안정성이 떨어지기 때문에 미리 올려 둔 S3에 있는 모델을 가져다가 학습하도록 수정을 하였습니다.\n",
    "\n",
    "### 로컬 디버깅\n",
    "\n",
    "- SageMaker training job을 실행하기에 앞서 local debugging을 통해 학습 코드가 정상적으로 구성되었는지 체크할 수 있습니다.\n",
    "- 로컬 디버깅 스크립트는 `sft-src/local_debug.sh` 파일을 참고해 주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b027573-da4f-48b5-be13-2a6b39aefd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "job_name = name_from_base(\"dolly-peft-sft-train\")\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00ffce-74ed-4e93-8d39-29c4355e5403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.p3.2xlarge'\n",
    "# instance_type = 'ml.g5.4xlarge'\n",
    "# instance_type = 'ml.g4dn.4xlarge'\n",
    "\n",
    "hyperparameters ={                               \n",
    "  'pretrain_model_path': '/opt/ml/input/data/pretrained-model',  # pretrained model from s3 will be located\n",
    "  'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                    # batch size for training\n",
    "  'lr': 2e-4,                                          # learning rate used during training\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_peft_train.py',      # train script\n",
    "    source_dir           = 'sft-src',         # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059ded2-b3a4-49ff-8444-31cdd21ef3c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 학습 시작\n",
    "\n",
    "- [HuggingFace estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) 를 정의했으므로 학습을 시작할 수 있습니다.\n",
    "- 이 때, 아래에서 `fit()` 함수를 호출해서 학습을 시작할 때 training data s3 위치 뿐 아니라 pretrained model 의 s3 uri 도 넣어주게 됩니다.\n",
    "- 아래와 같이 했을 때 `SM_CHANNEL_PRETRAINED-MODEL` : `/opt/ml/input/data/pretrained-model` 환경변수 값이 들어가게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700b6e7-dc64-4041-96f1-ba72e6f2ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_uri = model_artifact\n",
    "data = {'training': training_input_path, 'pretrained-model': pretrained_uri}\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec32779-52fd-467c-b6c9-d193cc97afbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit(data, wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d200a-5531-4c8f-8f3a-086d351ae928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f423d207-7660-4320-ba42-aab687135018",
   "metadata": {},
   "source": [
    "### 배포 및 테스트 진행\n",
    "\n",
    "- 학습은 `g5.4xlarge`로 대략 6시간 정도 걸립니다.\n",
    "- 학습 완료 후 아래와 같이 배포해서 테스트 가능합니다.\n",
    "- training job은 원격에서 진행됨. kernel session이 끊겨도 아래처럼 attach() 해서 가져올 수 있습니다.\n",
    "- 일반적으로 LLM 배포는 DJL을 사용하는 것이 좋지만, compressed size가 30GB 보다 훨씬 적고 inference code 도 포함되어 있다면 그냥 [HuggingFaceModel](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model) 로 배포해도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1eb27-db2d-46ba-bf53-802216562772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "training_job_name = \"dolly-peft-train-2023-04-24-10-16-54-73-2023-04-24-10-17-14-290\"\n",
    "estimator = Estimator.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15690de7-46a8-4d93-8b9a-20d19907c8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98671e-f5ce-4c1a-94f0-a9fabb7e9af3",
   "metadata": {},
   "source": [
    "### 학습된 모델 배포하기\n",
    "\n",
    "학습 후 배포에 필요한 model.tar.gz 의 구조의 예시는 다음과 같습니다.\n",
    "- code가 포함되어 있는데 이렇게 하는 것이 시간 상 더 효율적입니다. inference 용도 스크립트를 따로 명시하면 `기존 모델 decompress -> 코드 추가하여 compress -> s3 upload` 과정을 다시 진행하기 때문입니다.\n",
    "- 스크립트는 `code 디렉토리` 내에 들어가고 상위 폴더에 model 들이 있는 형태이지만, model은 특정 디렉토리에 모아놓고 inference code에서 해당 디렉토리에 있는 모델을 사용하도록 변경해주는 형태로 사용해도 상관 없습니다.\n",
    "```\n",
    "- code\n",
    "  - inference.py\n",
    "  - requirements.txt\n",
    "- config.json\n",
    "- tokenizer.json\n",
    "- tokenizer_config.json\n",
    "- pytorch_model_xxx.bin\n",
    "- special_tokens_map.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036b712-10e1-4849-8f38-13937e85c5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=estimator.model_data,\n",
    "   #model_data=\"s3://hf-sagemaker-inference/model.tar.gz\",  # model path 직접 주어도 됩니다.\n",
    "   role=role, \n",
    "   transformers_version=\"4.26\", \n",
    "   pytorch_version=\"1.13\", \n",
    "   py_version=\"py39\",\n",
    "   model_server_workers=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908284c9-bca8-492e-85cf-335e67c1857d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type= \"ml.g5.4xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c529c-2348-4e2b-86a7-88168f7cea18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 배포된 모델 테스트\n",
    "\n",
    "- `samsum` dataset의 test set을 받아서 테스트 진행해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083f41e-38f9-424e-81e9-3bf603e7b414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b701f-5c40-4bfb-b018-77797be910fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# select a random test sample\n",
    "sample = test_dataset[randint(0,len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "fomatted_sample = {\n",
    "  \"inputs\": prompt_template.format(dialogue=sample[\"dialogue\"]),\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_new_tokens\": 100,\n",
    "  }\n",
    "}\n",
    "\n",
    "print(fomatted_sample[\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1ec0a-bfa7-4d2d-98fa-20e5275aec36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# predict\n",
    "res = predictor.predict(fomatted_sample)\n",
    "\n",
    "output = res[0][\"generated_text\"].split(\"Summary:\")[-1]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a11ec2-e656-44ff-9b7f-299ab2ec3a1e",
   "metadata": {},
   "source": [
    "## 결과 비교\n",
    "- SFT를 안했을 때와 비교해 보면 대화 요약에 대해서 성능이 향상되었음을 알 수 있습니다.\n",
    "\n",
    "\n",
    "## 질문1\n",
    "```\n",
    "Summarize the chat dialogue:\n",
    "John: hey laurel?\n",
    "Laurel: hey \n",
    "John: whats your plan for tomorrow?\n",
    "Laurel: aint that sure yet, why?\n",
    "John: nothing much, just wanted to go with you and buy a birthday gift for Diana.\n",
    "Laurel: OMG! i also totally forgot that her birthday is on saturday, shit!\n",
    "John: you see im not the only late one here. haha\n",
    "Laurel: I guess we can meet up tomorrow and go fetch something for her.\n",
    "John: cool, at what time?\n",
    "Laurel: lets just meet at jades at around 5 pm\n",
    "John: At Jade's collection? in town?\n",
    "Laurel: yeah, that place..\n",
    "John: see you then.\n",
    "---\n",
    "Summary:\n",
    "```\n",
    "\n",
    "### base답변1\n",
    "```\n",
    "Laurel: hey \n",
    "John: whats your plan for tomorrow?\n",
    "Laurel: aint that sure yet, why?\n",
    "John: nothing much, just wanted to go with you and buy a birthday gift for Diana.\n",
    "Laurel: OMG! i also totally forgot that her birthday is on saturday, shit!\n",
    "John: you see im not the only late one here. haha\n",
    "Laurel: I guess we can meet up tomorrow and go fetch something for her.\n",
    "John: cool, at what time?\n",
    "Laurel: lets just meet at jades at around 5 pm\n",
    "John: At Jade's collection? in town?\n",
    "Laurel: yeah, that place..\n",
    "John: see you then.\n",
    "```\n",
    "\n",
    "### sft답변1\n",
    "```\n",
    "John and Laurel are going to meet at Jade's collection at 5 pm to buy a birthday gift for Diana.\n",
    "```\n",
    "\n",
    "## 질문2\n",
    "```\n",
    "Summarize the chat dialogue:\n",
    "Avery: You went to Ethan's house?\n",
    "David: yeah I had to babysit\n",
    "Avery: Aww, how do you babysit, just curious\n",
    "David: I had to go through a lot :/\n",
    "Avery: Was his sister naughty\n",
    "David: Tooo much\n",
    "Avery: Lol\n",
    "David: I will just refuse net time :/\n",
    "Avery: As you wish\n",
    "David: :/\n",
    "Avery: I just got his text \n",
    "David: What is he saying\n",
    "Avery: He is asking me to say thanks to you\n",
    "David: yeah whatever<3\n",
    "Avery: He was saying that your phone was switched off\n",
    "David: Yeah i have just turned it on\n",
    "Avery: I have told him about that\n",
    "David: k\n",
    "Avery: Gotta go now\n",
    "---\n",
    "Summary:\n",
    "```\n",
    "\n",
    "### base답변2\n",
    "```\n",
    "- I went to Ethan's house\n",
    "- His sister was naughty\n",
    "- I had to babysit her\n",
    "- She was too much\n",
    "- I had to turn off his phone\n",
    "- I just turned it on\n",
    "```\n",
    "\n",
    "### sft답변2\n",
    "```\n",
    "David babysits Ethan's sister. Ethan asked David to thank him for the babysitting. David turned his phone on.\n",
    "```\n",
    "\n",
    "\n",
    "## 질문3\n",
    "```\n",
    "Summarize the chat dialogue:\n",
    "Paul: Do you have your keys?\n",
    "Lena: Yup. What's up?\n",
    "Paul: I will be home later than I thought, so just don't wait for me and get home.\n",
    "Lena: What happened?\n",
    "Paul: I'll call you in 15 minutes and then I'll tell you.\n",
    "Lena: Ok.\n",
    "---\n",
    "Summary:\n",
    "```\n",
    "\n",
    "### base답변3\n",
    "```\n",
    "Lena: Yup. What's up?\n",
    "Paul: I will be home later than I thought, so just don't wait for me and get home.\n",
    "Lena: What happened?\n",
    "Paul: I'll call you in 15 minutes and then I'll tell you.\n",
    "Lena: Ok.\n",
    "---\n",
    "Paul: I will be home later than I thought, so just don't wait for me and get home.\n",
    "Lena: What happened?\n",
    "Paul: I'll call you in 15 minutes and then I'll tell you.\n",
    "Lena: Ok.\n",
    "---\n",
    "Paul: I'll tell you in 15 minutes.\n",
    "Lena: Ok.\n",
    "Paul: Bye!\n",
    "```\n",
    "\n",
    "### sft답변3\n",
    "```\n",
    "Paul will be home later than he thought. Lena should not wait for him. Paul will call her in 15 minutes.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16079414-d67a-49b5-b7b4-6f959fd39f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
