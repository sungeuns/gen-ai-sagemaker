{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a0200f-29b4-4a92-8175-87ebe275d737",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLHF (Reinforcement Learning with Human Feedback)를 활용하여 원하는 방향으로 성능 향상시키기\n",
    "\n",
    "- RLHF는 human feedback 데이터를 모아서, 여기서 reward model 을 학습시킨 후 이 reward model 을 사용하여 [PPO](https://huggingface.co/learn/deep-rl-course/unit8/introduction?fw=pt] 알고리즘으로 RL 하여 모델 성능을 향상시키는 방법입니다. human feedback 데이터가 원하는 방향으로 모델이 동작하도록 만드는 방식입니다.\n",
    "- 이 예시에서는 TRL/PEFT 기반으로 소량의 GPU 리소스로 학습하는 [블로그](https://huggingface.co/blog/trl-peft)를 참고하였습니다.\n",
    "- 해당 글에서는 reward model을 따로 학습하지는 않고, 그냥 별도의 imbd dataset에 BERT 기반으로 sentimental classification하는 [모델](https://huggingface.co/lvwerra/distilbert-imdb)을 가져다가 활용하였습니다. \n",
    "- 코드 참고 : https://github.com/lvwerra/trl/tree/main/examples/sentiment/scripts/gpt-neox-20b_peft\n",
    "- 앞의 예시에서 SFT를 위해 학습을 진행하는 것과 거의 유사합니다. 실제 학습을 위한 스크립트만 변경되고 SageMaker 사용법은 거의 동일합니다.\n",
    "\n",
    "### Tested version\n",
    "\n",
    "Tested on `Python 3.9.15`\n",
    "\n",
    "```\n",
    "sagemaker: 2.146.0\n",
    "transformers: 4.29.2\n",
    "torch: 1.13.1\n",
    "accelerate: 0.19.0\n",
    "datasets: 2.12.0\n",
    "py7zr: 0.20.5\n",
    "peft: 0.3.0\n",
    "bitsandbytes: 0.38.1\n",
    "trl: 0.4.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3df905-ef20-4577-94eb-4d775f89c60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets py7zr trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17384adb-58a5-44d7-b847-2dd076ab634d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# peft 0.3 부터 model merge 함수가 있기 때문에, model merge를 쉽게 하기위해 0.3 버전을 활용합니다.\n",
    "!pip install -q peft==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605bafa-c634-4f95-ae22-6efa6eff0c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import sagemaker\n",
    "import peft\n",
    "print(transformers.__version__)\n",
    "print(sagemaker.__version__)\n",
    "print(peft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431c8e9-e689-43a0-8b4d-4c39cacc1441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "sm_runtime_client = sagemaker_session.sagemaker_runtime_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecbb91-af3a-4c50-b32b-fa3c4e421069",
   "metadata": {},
   "source": [
    "### 데이터셋 준비\n",
    "\n",
    "- 앞의 예시와 동일하게 IMDB dataset을 준비하고 이를 s3에 업로드 해놓도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac166df5-11d4-4a8a-bd7a-bbb0d6c1bfe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f45f5-8f5b-4b41-844a-da783d133224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe51adb-ffe5-40a3-8783-e91b69afef55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(model_path, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3fdba-b7c7-4804-8aa8-4559f22afe0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_location = model_download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9f96a-f4c1-4973-b118-3abbb4ac47f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = build_dataset(model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e92728-c8e7-4684-bee0-0bcebb9b57f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "print(dataset[randint(0, len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2bb8c-87b2-4883-84d2-93299a132abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_input_path = f\"s3://{sagemaker_session.default_bucket()}/llm/databricks/dolly-v2-7b/dataset/imdb\"\n",
    "dataset.save_to_disk(training_input_path)\n",
    "print(f\"Data uploaded : {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73b6f8-87ad-4569-ab9a-9501262bc054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"./imdb-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c13ef9-ad27-4daa-9fe5-2ef6ee18acad",
   "metadata": {},
   "source": [
    "### 학습 진행\n",
    "\n",
    "- 학습 방법은 앞의 fine-tuning 예시와 거의 동일합니다.\n",
    "- `rlhf-src` 디렉토리를 참고해 주세요.\n",
    "\n",
    "### 로컬 디버깅 방법\n",
    "\n",
    "- 학습 데이터의 경우 S3에 곧바로 데이터를 업로드 했지만, local storage에도 저장해 놓고 디버깅에 활용할 수 있습니다.\n",
    "- 실제 개발환경에서는 곧바로 SageMaker training job을 던지기보다는 local mode로 충분히 테스트 후 job을 던지는 형태로 진행하게 됩니다.\n",
    "- 따라서, 아래 예시 처럼 먼저 local debugging을 해 보고 training을 하는 것이 좋습니다.\n",
    "```\n",
    "python run_rlhf_train.py --dataset_path [로컬_데이터셋_경로] --model_name {model_artifact}\n",
    "```\n",
    "- 로컬 디버깅 스크립트는 `rlhf-src/local_debug.sh` 를 참고해 주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13300f28-92e4-42ba-af50-e358ff92e9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "job_name = name_from_base(\"dolly-rlhf-train\")\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ef432-58cd-4772-8de3-19d0dadf34d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.g5.4xlarge'\n",
    "# instance_type = 'ml.g4dn.4xlarge'\n",
    "\n",
    "hyperparameters ={                         \n",
    "  'model_name': '/opt/ml/input/data/pretrained-model',\n",
    "  'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "  # 'mini_batch_size': 16,\n",
    "  # 'batch_size': 256,\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_rlhf_train.py',      # train script\n",
    "    source_dir           = 'rlhf-src',         # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51606f92-3c9d-4a2f-a2cf-3c8de48a83a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_uri = \"s3://sagemaker-us-west-2-723597067299/llm/databricks/dolly-v2-7b/model/\"\n",
    "# pretrained_uri = model_artifact\n",
    "data = {'training': training_input_path, 'pretrained-model': pretrained_uri}\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeaaf36-251a-4176-88b0-e717147549cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit(data, wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9268c-e992-4064-b850-090e1c5e7ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d337ddc-6a0e-46a0-8341-796123e63c91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 배포 및 테스트 진행\n",
    "\n",
    "- 학습은 `g5.4xlarge`로 대략 10시간 정도 걸립니다.\n",
    "- 학습 완료 후 아래와 같이 배포해서 테스트 가능합니다.\n",
    "- 이전 예시처럼, kernel session이 끊겨도 아래처럼 attach() 해서 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e07956-ba2e-4238-ad43-8138e1da9c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "training_job_name = \"dolly-rlhf-train-2023-04-25-15-41-52-29-2023-04-25-15-42-00-690\"\n",
    "estimator = Estimator.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644029b2-0c5b-49cd-b2e7-736fc80195ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41a6db-df71-4d7f-a414-a506906f075a",
   "metadata": {},
   "source": [
    "### 모델 병합\n",
    "\n",
    "- 학습이 정상적으로 완료되면 low rank adapter 부분이 s3 에 저장됩니다. 앞의 예시에서는 학습이 완료되면 모델을 병합하여 올리도록 했지만, 이번 예시는 adapter 부분만 업로드 합니다.\n",
    "- 해당 adapter는 원본 모델에 merge해서 사용할 수 있습니다 : [참고]( https://github.com/lvwerra/trl/blob/main/examples/sentiment/scripts/gpt-neox-20b_peft/merge_peft_adapter.py)\n",
    "- peft 0.3 버전에서는 `merge_and_unload()` 함수를 사용할 수 있습니다: [참고](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L299)\n",
    "- 따라서 여기서는 git을 통해 최신 릴리즈의 lora 설치 후 해당 함수를 활용해 병합하는 예시를 보여줍니다.\n",
    "- 별도로 병합하는 것이 번거로울 수 있어서 학습 코드 자체에 학습이 완료된 후 base model 과 adapter를 병합해서 s3에 저장하도록 해도 됩니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a4824-f012-4754-b978-bd1c95522b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1167cd-04fa-4e0f-aefc-d50a017a1aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_path = \"./dolly-imdb-finetune\"\n",
    "adapter_path = f\"{finetune_path}/adapter\"\n",
    "print(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61995c7f-8c82-4f11-b624-49e5ab3646ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {adapter_path}\n",
    "!aws s3 cp {estimator.model_data} {adapter_path}\n",
    "!cd {adapter_path}; tar zxvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7742ee-2b75-4433-82a7-3c296dc3f202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_location,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94af61f-005e-47c3-97c3-27b5989af8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(finetune_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a552f-dc8f-4846-8c87-a362c0227e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "tokenizer.save_pretrained(finetune_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5523288-824f-4acf-b876-0e99ec1df55e",
   "metadata": {},
   "source": [
    "\n",
    "### 병합된 모델 테스트\n",
    "\n",
    "- merge된 모델은 local mode로 로딩하거나, endpoint로 배포해서 사용할 수 있습니다.\n",
    "- 최종 merge된 모델을 로딩해서 질문을 하면 아래 예시와 같이 거의 항상 positive한 답변을 하게 되는 것을 확인할 수 있습니다.\n",
    "\n",
    "## 질문1\n",
    "Explain to me how to use aws serverless services\n",
    "\n",
    "### base답변1\n",
    "You can use serverless framework to create functions in various languages like node.js, java, c#, go, python etc. These functions are hosted on S3 objects or anything that is exposed by serverless framework in the form of acloudfunction. The way you deploy these functions is by registering a region and a package where the functions and dependencies are stored. A separate deployment package is generated for each function. You can refer this link for more details on serverless framework and serverless functions.\n",
    "\n",
    "### rlhf답변1\n",
    "serverless is a very powerful and flexible platform. The best way to get started is to go through their comprehensive documentation. A good starting point is the Playground which allows you to quickly get started with different services.\n",
    "Enjoy exploring!\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "## 질문2\n",
    "How do you feel after watching the avartar 2?\n",
    "\n",
    "### base답변2\n",
    "I felt like watching a action packed movie. The action sequences were good, and there's lots of gun fights, car chases, and fight rigging sequences. The lead actors did a great job. Also, the music and cinematography were top notch.\n",
    "\n",
    "### rlhf답변2\n",
    "The movie is amazing, very well made and really brings life to the story. The cast is amazing as always. Overall highly recommended!\n",
    "\n",
    "----\n",
    "\n",
    "## 질문3\n",
    "I heard that the disney movie is very bad recently. What do you think?\n",
    "\n",
    "### base답변2\n",
    "The movie has so many characters. Different styles of animation andDirection. I think that is what make the movie look so wild. I also like the love story that includes hero and heroine in high school and the make out in the end, very love story sweet. Also the touching moment between children. Overall, I think it is a good movie to watch with family. If you are a child, you can watch it yourself.\n",
    "\n",
    "### rlhf답변3\n",
    "I totally disagree with you. I loved the disney movie. It is so good! The story, the characters, the music, the animation, the effects are all so perfect. I love it. I also love the new disney movie. I love the old, too, but this one is the best! It is so good! I love it!\n",
    "\n",
    "----\n",
    "\n",
    "## 질문4\n",
    "I hate sam, what do you think about him?\n",
    "\n",
    "### base답변4\n",
    "I hate sam is a subjective opinion, therefore I can not express hate for him. There are many people that love him, and hate him. I do not like him because he is a bully, he has an ego, and he lies, he is not honest. He is a con artist.\n",
    "\n",
    "### rlhf답변4\n",
    "I hate sam, what do you think about him?\n",
    "I totally agree with you. He is a rude, selfish and annoying person, but he has such a great heart. He always try to help others and be kind to everyone. I love his attitude towards life. I admire him.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933520bd-cc16-4c48-94cf-ede6efafe87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
